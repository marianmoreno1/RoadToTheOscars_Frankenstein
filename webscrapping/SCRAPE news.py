import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class GoogleNewsSimpleScraper:
    def __init__(self, output_path):
        self.output_path = output_path
        self.all_articles = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'es-ES,es;q=0.9',
        }
    
    def scrape_google_news_rss(self, query, max_results=50):
        """Extrae noticias usando Google News RSS (mÃ¡s confiable)"""
        articles = []
        
        try:
            logger.info(f"ðŸ” Buscando: {query}")
            
            # Usar el RSS de Google News (cambiado a regiÃ³n USA)
            rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
            
            response = requests.get(rss_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')
            
            logger.info(f"âœ… Encontrados {len(items)} artÃ­culos en RSS")
            
            for idx, item in enumerate(items[:max_results], 1):
                try:
                    title = item.find('title').text if item.find('title') else "N/A"
                    link = item.find('link').text if item.find('link') else "N/A"
                    pub_date = item.find('pubDate').text if item.find('pubDate') else "N/A"
                    source = item.find('source').text if item.find('source') else "Desconocida"
                    
                    article_data = {
                        "Titular": title,
                        "Fuente": source,
                        "Fecha": pub_date,
                        "URL": link,
                        "Query_Busqueda": query,
                        "Fecha_Extraccion": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    
                    articles.append(article_data)
                    
                    if idx % 10 == 0:
                        logger.info(f"   âœ“ Procesados {idx} artÃ­culos...")
                
                except Exception as e:
                    logger.debug(f"   âœ— Error en artÃ­culo {idx}: {e}")
                    continue
            
            logger.info(f"ðŸ“Š Total extraÃ­dos: {len(articles)} artÃ­culos\n")
            
        except Exception as e:
            logger.error(f"âŒ Error en bÃºsqueda '{query}': {e}\n")
        
        return articles
    
    def scrape_multiple_queries(self, queries, max_per_query=50):
        """Scrape mÃºltiples bÃºsquedas"""
        logger.info("="*70)
        logger.info("ðŸš€ INICIANDO EXTRACCIÃ“N DE GOOGLE NEWS (RSS)")
        logger.info("="*70 + "\n")
        
        for idx, query in enumerate(queries, 1):
            logger.info(f"{'='*70}")
            logger.info(f"ðŸ” BÃšSQUEDA {idx}/{len(queries)}: {query}")
            logger.info(f"{'='*70}\n")
            
            try:
                articles = self.scrape_google_news_rss(query, max_per_query)
                self.all_articles.extend(articles)
                logger.info(f"âœ… Completada bÃºsqueda {idx}/{len(queries)}\n")
                time.sleep(2)  # Pausa entre bÃºsquedas
            except Exception as e:
                logger.error(f"âŒ Error en bÃºsqueda {idx}: {e}\n")
                continue
        
        # Crear DataFrame y eliminar duplicados
        df = pd.DataFrame(self.all_articles)
        
        if len(df) > 0:
            original_count = len(df)
            df = df.drop_duplicates(subset=["Titular"], keep="first")
            duplicates_removed = original_count - len(df)
            
            logger.info("="*70)
            logger.info("âœ… EXTRACCIÃ“N COMPLETADA")
            logger.info("="*70)
            logger.info(f"   ðŸ“° Total artÃ­culos extraÃ­dos: {original_count}")
            logger.info(f"   ðŸ—‘ï¸  Duplicados eliminados: {duplicates_removed}")
            logger.info(f"   âœ¨ ArtÃ­culos Ãºnicos: {len(df)}")
            logger.info(f"   ðŸ“š Fuentes Ãºnicas: {df['Fuente'].nunique()}")
            
            # Guardar
            df.to_excel(self.output_path, index=False)
            logger.info(f"\nðŸ’¾ Archivo guardado en: {self.output_path}")
            
            return df
        else:
            logger.warning("\nâŒ No se encontraron noticias")
            return pd.DataFrame()


if __name__ == "__main__":
    # ConfiguraciÃ³n
    output_path = "/Users/allende/Desktop/5 ICAI /Segundo cuatri/AnalÃ­tica Social y de la Web /Trabajo PrÃ¡ctico /IMDB Y ROTTENTOMATOES/Frankenstein_Google_News_RSS.xlsx"
    
    # Queries de bÃºsqueda en inglÃ©s (enfocadas a USA/Oscars)
    queries = [
        "Frankenstein 2025 movie Guillermo del Toro",
        "Frankenstein Jacob Elordi Oscar nomination",
        "Frankenstein Netflix awards season",
        "Frankenstein Venice Film Festival reviews",
        "Guillermo del Toro Frankenstein Academy Awards",
        "Frankenstein 2025 box office performance",
        "Frankenstein movie awards predictions",
        "Frankenstein del Toro critical reception"
    ]
    
    # Crear scraper y ejecutar
    scraper = GoogleNewsSimpleScraper(output_path)
    df_news = scraper.scrape_multiple_queries(queries, max_per_query=100)
    
    # Mostrar muestra de resultados
    if len(df_news) > 0:
        print("\n" + "="*70)
        print("ðŸ“° MUESTRA DE RESULTADOS (primeros 10)")
        print("="*70)
        print(df_news[['Titular', 'Fuente', 'Fecha']].head(10))
        print("\n" + "="*70)
        print("ðŸ“Š DISTRIBUCIÃ“N POR FUENTE")
        print("="*70)
        print(df_news['Fuente'].value_counts().head(10))